{"cells":[{"cell_type":"code","execution_count":1,"id":"OsNax0LNffEw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43837,"status":"ok","timestamp":1749117273215,"user":{"displayName":"Ïù¥Ïû¨ÏõÖ","userId":"11513447611166287348"},"user_tz":-540},"id":"OsNax0LNffEw","outputId":"6c578151-3a74-43dd-f7b7-615fa2165834"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","\n","# Ï†ïÌôïÌïú ÌååÏùº Í≤ΩÎ°ú\n","file_path = '/content/drive/MyDrive/current_df.csv'\n","\n","# Ïù∏ÏΩîÎî© ÌôïÏù∏ (Í∏∞Î≥∏ÏùÄ utf-8, Ïïà ÎêòÎ©¥ cp949)\n","df = pd.read_csv(file_path, encoding='utf-8')\n"]},{"cell_type":"code","execution_count":null,"id":"027b9f4c","metadata":{"colab":{"background_save":true},"id":"027b9f4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Loss: 34864.8973\n","Epoch 2 Loss: 13901.8265\n","Epoch 3 Loss: 12575.1814\n"]}],"source":["\n","# üß™ Deep Attention Regressor for Missing Value Imputation (ta)\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","\n","# ‚úÖ Step 2: Define Features and Target\n","features = ['wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'ta_chi', 'congestion']\n","target = 'ta'\n","\n","# ‚úÖ Step 3: Handle Missing Values\n","df[features + [target]] = df[features + [target]].replace(-99, np.nan)\n","train_df = df[df[target].notna()].copy()\n","test_df = df[df[target].isna()].copy()\n","\n","X_train = train_df[features].fillna(0)\n","y_train = train_df[target]\n","X_test = test_df[features].fillna(0)\n","\n","# ‚úÖ Step 4: Convert to Tensors\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","y_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n","X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","\n","train_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=2048, shuffle=True)\n","\n","# ‚úÖ Step 5: Define Deep Attention Model\n","class DeepAttentionRegressor(nn.Module):\n","    def __init__(self, input_dim, d_model=128, num_heads=4, num_layers=3, ff_dim=256, dropout=0.1):\n","        super().__init__()\n","        self.input_fc = nn.Linear(input_dim, d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            dim_feedforward=ff_dim,\n","            dropout=dropout,\n","            batch_first=True,\n","            activation='gelu'\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.output_head = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_model // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model // 2, 1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.input_fc(x).unsqueeze(1)\n","        x = self.transformer(x)\n","        x = x.squeeze(1)\n","        return self.output_head(x).squeeze(1)\n","\n","# ‚úÖ Step 6: Train Model\n","model = DeepAttentionRegressor(input_dim=len(features)).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","criterion = nn.MSELoss()\n","loss_history = []\n","\n","for epoch in range(20):\n","    model.train()\n","    total_loss = 0\n","    for xb, yb in train_loader:\n","        optimizer.zero_grad()\n","        pred = model(xb)\n","        loss = criterion(pred, yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    loss_history.append(total_loss)\n","    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n","\n","# ‚úÖ Step 7: Predict and Fill Missing Values\n","model.eval()\n","with torch.no_grad():\n","    ta_pred = model(X_test_tensor).cpu().numpy()\n","\n","df.loc[df['ta'].isna(), 'ta'] = ta_pred\n","\n","# ‚úÖ Step 8: Visualize Training Loss\n","plt.plot(loss_history)\n","plt.title(\"Training Loss (MSE)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","id":"0nOjVB-fefw4","metadata":{"id":"0nOjVB-fefw4"},"source":["# ÏÉà ÏÑπÏÖò"]},{"cell_type":"code","execution_count":null,"id":"cabc2c08","metadata":{"id":"cabc2c08"},"outputs":[],"source":["\n","# ‚úÖ Fast GPU Attention Training (for ta imputation)\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","\n","# ÌÖêÏÑú Î≥ÄÌôò Î∞è GPU Ïù¥Îèô\n","X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","y_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n","X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","\n","# Í≤ΩÎüâ Ïñ¥ÌÖêÏÖò Î™®Îç∏ Ï†ïÏùò\n","class FastAttentionRegressor(nn.Module):\n","    def __init__(self, input_dim, d_model=32, num_heads=1, num_layers=1, ff_dim=64, dropout=0.1):\n","        super().__init__()\n","        self.input_fc = nn.Linear(input_dim, d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model,\n","            nhead=num_heads,\n","            dim_feedforward=ff_dim,\n","            dropout=dropout,\n","            batch_first=True,\n","            activation=\"gelu\"\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.output_head = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_model // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model // 2, 1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.input_fc(x).unsqueeze(1)\n","        x = self.transformer(x)\n","        x = x.squeeze(1)\n","        return self.output_head(x).squeeze(1)\n","\n","# Îç∞Ïù¥ÌÑ∞Î°úÎçî Î∞è ÌïôÏäµ Íµ¨ÏÑ±\n","train_ds = TensorDataset(X_tensor, y_tensor)\n","train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n","\n","model = FastAttentionRegressor(input_dim=X_tensor.shape[1]).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","loss_history = []\n","for epoch in range(5):\n","    model.train()\n","    total_loss = 0\n","    loop = tqdm(train_loader, desc=f\"[Epoch {epoch+1}/5]\", leave=False)\n","    for xb, yb in loop:\n","        optimizer.zero_grad()\n","        pred = model(xb)\n","        loss = criterion(pred, yb)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        loop.set_postfix(loss=loss.item())\n","    loss_history.append(total_loss)\n","    print(f\"‚úÖ Epoch {epoch+1} | Total Loss: {total_loss:.4f}\", flush=True)\n","\n","# Í≤∞Ï∏°Ïπò ÏòàÏ∏° Î∞è Î∞òÏòÅ\n","model.eval()\n","with torch.no_grad():\n","    ta_pred = model(X_test_tensor).cpu().numpy()\n","\n","df.loc[df[\"ta\"].isna(), \"ta\"] = ta_pred\n"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}